// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert DATATYPE in ["F32"]
$assert ACTIVATION in ["LINEAR", "RELU", "MINMAX"]
$assert MR >= 1
$assert NR in ["m1", "m2", "m4", "m8"]
$LMUL = NR[1]
#include <assert.h>

#include <riscv_vector.h>

#include "xnnpack/gemm.h"


$DATATYPE_SPEC = {"F32": "f32"}[DATATYPE]
$SUFFIX = {"LINEAR": "", "RELU": "_relu", "MINMAX": "_minmax"}[ACTIVATION]
$PARAMS = {"LINEAR": "union xnn_f32_default_params", "RELU": "union xnn_f32_relu_params", "MINMAX": "union xnn_f32_minmax_params"}[ACTIVATION]
void xnn_${DATATYPE_SPEC}_transpose_a_pruned_gemm${SUFFIX}_ukernel_${MR}x${LMUL}v__rvv(
    size_t mr,
    size_t nc,
    size_t kc,
    const float* restrict a,
    size_t w_stride,
    const float*  w,
    const float*  bias,
    float*  c,
    size_t cm_stride,
    size_t cn_stride,
    uint16_t* indice,
    const ${PARAMS} params[restrict XNN_MIN_ELEMENTS(1)])
{
  assert(mr != 0);
  assert(mr <= ${MR});
  assert(nc != 0);
  assert(kc != 0);
  assert(kc % sizeof(float) == 0);
  assert(a != NULL);
  assert(bias != NULL);
  assert(w != NULL);
  assert(c != NULL);

  $if ACTIVATION == "MINMAX":
    const float vmin = params->scalar.min;
    const float vmax = params->scalar.max;
  $elif ACTIVATION == "RELU":
    const float vmin = 0.0f;
  const float* w0 = w;
  float* c0 = c;
  const float* bias0 = bias;
  $for M in range(1, MR):
    const float* w${M} = (const float*) ((uintptr_t) w${M-1} + w_stride);
    float* c${M} = (float*) ((uintptr_t) c${M-1} + cm_stride);
    const float* bias${M} = (const float*) ((uintptr_t)bias${M-1} + sizeof(float));
    $if M % 2 == 0:
      if XNN_UNPREDICTABLE(mr <= ${M}) {
        w${M} = w${M-1};
        c${M} = c${M-1};
        bias${M} = bias${M-1};
      }
    $elif M + 1 == MR:
      if XNN_UNPREDICTABLE(mr != ${M+1}) {
        w${M} = w${M-1};
        c${M} = c${M-1};
        bias${M} = bias${M-1};
      }
    $else:
      if XNN_UNPREDICTABLE(mr < ${M+1}) {
        w${M} = w${M-1};
        c${M} = c${M-1};
        bias${M} = bias${M-1};
      }

  const size_t nr = __riscv_vsetvlmax_e32m${LMUL}();
  size_t vl = nr;
  do {
    if XNN_UNLIKELY(nc < nr) {
      vl = __riscv_vsetvl_e32m${LMUL}(nc);
    }
    nc = nc - vl;

    $for M in range(MR):  
      vfloat32m${LMUL}_t vacc${M} =  __riscv_vfmv_v_f_f32m${LMUL}(*bias${M}, vl);

    size_t k = kc;
    size_t idx_indice_arr = 0;
    do {
      $for M in range(MR):
        const float vw${M} = *w${M}++;
      vfloat32m4_t vb = __riscv_vle32_v_f32m4(a + indice[idx_indice_arr], vl);
      idx_indice_arr++;
      $for M in range(MR):
        vacc${M} = __riscv_vfmacc_vf_f32m${LMUL}(vacc${M}, vw${M}, vb, vl);
      k -= sizeof(float);
    } while (k != 0);
    $if ACTIVATION == "MINMAX":
      // clamp results with min & max
      $for M in range(MR):
        vacc${M} = __riscv_vfmax_vf_f32m${LMUL}(vacc${M}, vmin, vl);

      $for M in range(MR):
        vacc${M} = __riscv_vfmin_vf_f32m${LMUL}(vacc${M}, vmax, vl);
    $elif ACTIVATION == "RELU":
      // apply ReLU to results
      $for M in range(MR):
        vacc${M} = __riscv_vfmax_vf_f32m${LMUL}(vacc${M}, vmin, vl);
    // store ${MR} x vl results to c
    $for M in range(MR):  
      __riscv_vse32_v_f32m${LMUL}(c${M}, vacc${M}, vl);
      c${M} = (float*) ((uintptr_t) c${M} + cn_stride);
    $for M in range(MR):  
      w${M} = (const float*) ((uintptr_t) w${M} - kc);
  } while (nc != 0);
}
